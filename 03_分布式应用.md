<font size='7'>分布式应用</font>

# 1、分布式应用

## 1、简介

[百度百科](https://baike.baidu.com/item/%E5%88%86%E5%B8%83%E5%BC%8F%E5%BA%94%E7%94%A8/1372598?fr=aladdin)：distributed application 分布式应用由不同的运行于分离的运行环境下的组件构成的应用程序，通常是在不同的平台上通过网络互联起来。典型的分布式应用是二端（Client/Server），三端（client/middleware/server）和n端（client/multiple middleware/multiple server）



## 2、分布式应用的演进

[参考博客](https://www.zhihu.com/question/22764869)

### 系统架构演化历程-初始阶段架构

<img src="F:\Typora\images\13bd5a6612620fdf51c8987ab3eb1273_720w.jpg" alt="img"  />

初始阶段 的小型系统 应用程序、数据库、文件等所有的资源都在一台服务器上通俗称为LAMP

**特征：**
应用程序、数据库、文件等所有的资源都在一台服务器上。

**描述：**
通常服务器操作系统使用linux，应用程序使用PHP开发，然后部署在Apache上，数据库使用Mysql，汇集各种免费开源软件以及一台廉价服务器就可以开始系统的发展之路了。



### 系统架构演化历程-应用服务和数据服务分离

<img src="F:\Typora\images\c2ff4e51eec15231b2f69fe6a4038239_720w.jpg" alt="img"  />

好景不长，发现随着系统访问量的再度增加，webserver机器的压力在高峰期会上升到比较高，这个时候开始考虑增加一台webserver

特征：
应用程序、数据库、文件分别部署在独立的资源上。

描述：
数据量增加，单台服务器性能及存储空间不足，需要将应用和数据分离，并发处理能力和数据存储空间得到了很大改善。



### **系统架构演化历程-使用缓存改善性能**

<img src="F:\Typora\images\7f5890aefa3a065ea91baa33e928d59b_720w.jpg" alt="img"  />

特征：
数据库中访问较集中的一小部分数据存储在缓存服务器中，减少数据库的访问次数，降低数据库的访问压力。

描述：
系统访问特点遵循二八定律，即80%的业务访问集中在20%的数据上。
缓存分为本地缓存和远程分布式缓存，本地缓存访问速度更快但缓存数据量有限，同时存在与应用程序争用内存的情况。



### **系统架构演化历程-使用应用服务器集群**

<img src="F:\Typora\images\66b50d0767750b0ff30d00d13a2a1963_720w.jpg" alt="img"  />

在做完分库分表这些工作后，数据库上的压力已经降到比较低了，又开始过着每天看着访问量暴增的幸福生活了，突然有一天，发现系统的访问又开始有变慢的趋势了，这个时候首先查看数据库，压力一切正常，之后查看webserver，发现apache阻塞了很多的请求，而应用服务器对每个请求也是比较快的，看来 是请求数太高导致需要排队等待，响应速度变慢

特征：
多台服务器通过负载均衡同时向外部提供服务，解决单台服务器处理能力和存储空间上限的问题。

描述：
使用集群是系统解决高并发、海量数据问题的常用手段。通过向集群中追加资源，提升系统的并发处理能力，使得服务器的负载压力不再成为整个系统的瓶颈。



### 系统架构演化历程-数据库读写分离

<img src="F:\Typora\images\e8cb08ede220e1f41459374e3ef72f85_720w.jpg" alt="img"  />

享受了一段时间的系统访问量高速增长的幸福后，发现系统又开始变慢了，这次又是什么状况呢，经过查找，发现数据库写入、更新的这些操作的部分数据库连接的资源竞争非常激烈，导致了系统变慢

特征：
多台服务器通过负载均衡同时向外部提供服务，解决单台服务器处理能力和存储空间上限的问题。

描述：
使用集群是系统解决高并发、海量数据问题的常用手段。通过向集群中追加资源，使得服务器的负载压力不在成为整个系统的瓶颈。



### 系统架构演化历程-反向代理和CDN加速

<img src="F:\Typora\images\a2db8db8b2d7500adf6135c76d6323f9_720w.jpg" alt="img"  />

特征：
采用CDN和反向代理加快系统的 访问速度。

描述：
为了应付复杂的网络环境和不同地区用户的访问，通过CDN和反向代理加快用户访问的速度，同时减轻后端服务器的负载压力。CDN与反向代理的基本原理都是缓存。



### 系统架构演化历程-分布式文件系统和分布式数据库

<img src="F:\Typora\images\ea091b2c553a7771695a4c707b091668_720w.jpg" alt="img"  />

随着系统的不断运行，数据量开始大幅度增长，这个时候发现分库后查询仍然会有些慢，于是按照分库的思想开始做分表的工作

特征：
数据库采用分布式数据库，文件系统采用分布式文件系统。

描述：
任何强大的单一服务器都满足不了大型系统持续增长的业务需求，数据库读写分离随着业务的发展最终也将无法满足需求，需要使用分布式数据库及分布式文件系统来支撑。
分布式数据库是系统数据库拆分的最后方法，只有在单表数据规模非常庞大的时候才使用，更常用的数据库拆分手段是业务分库，将不同的业务数据库部署在不同的物理服务器上。



### 系统架构演化历程-使用NoSQL和搜索引擎

<img src="F:\Typora\images\7e25f96d31da26661c078b847c8acc44_720w.jpg" alt="img"  />

特征：
系统引入NoSQL数据库及搜索引擎。

描述：
随着业务越来越复杂，对数据存储和检索的需求也越来越复杂，系统需要采用一些非关系型数据库如NoSQL和分数据库查询技术如搜索引擎。应用服务器通过统一数据访问模块访问各种数据，减轻应用程序管理诸多数据源的麻烦。



### 系统架构演化历程-业务拆分

![img](F:\Typora\images\51faaff90df43279c82ffd6a6b587135_720w.jpg)

特征：
系统上按照业务进行拆分改造，应用服务器按照业务区分进行分别部署。

描述：
为了应对日益复杂的业务场景，通常使用分而治之的手段将整个系统业务分成不同的产品线，应用之间通过超链接建立关系，也可以通过消息队列进行数据分发，当然更多的还是通过访问同一个数据存储系统来构成一个关联的完整系统。

纵向拆分：
将一个大应用拆分为多个小应用，如果新业务较为独立，那么就直接将其设计部署为一个独立的Web应用系统

纵向拆分相对较为简单，通过梳理业务，将较少相关的业务剥离即可。

横向拆分：将复用的业务拆分出来，独立部署为分布式服务，新增业务只需要调用这些分布式服务

横向拆分需要识别可复用的业务，设计服务接口，规范服务依赖关系。



### 系统架构演化历程-分布式服务

![img](F:\Typora\images\9ef9ad51a11b36cacb2f408ceb928c82_720w.jpg)

特征：
公共的应用模块被提取出来，部署在分布式服务器上供应用服务器调用。

描述：
随着业务越拆越小，应用系统整体复杂程度呈指数级上升，由于所有应用要和所有数据库系统连接，最终导致数据库连接资源不足，拒绝服务。



## 3、Java分布式应用技术基础

![img](F:\Typora\images\b7cdb7ac16e2719f5fefe9503f85ad69_720w.jpg)



# 2、Zookeeper

[W3C讲解](https://www.w3cschool.cn/zookeeper/zookeeper_overview.html)

## 1、概述

ZooKeeper是一种分布式协调服务，用于管理大型主机。在分布式环境中协调和管理服务是一个复杂的过程。ZooKeeper通过其简单的架构和API解决了这个问题。ZooKeeper允许开发人员专注于核心应用程序逻辑，而不必担心应用程序的分布式特性。

ZooKeeper框架最初是在“Yahoo!"上构建的，用于以简单而稳健的方式访问他们的应用程序。 后来，Apache ZooKeeper成为Hadoop，HBase和其他分布式框架使用的有组织服务的标准。 例如，Apache HBase使用ZooKeeper跟踪分布式数据的状态。



### 1、分布式应用

分布式应用可以在给定时间（同时）在网络中的多个系统上运行，通过协调它们以快速有效的方式完成特定任务。通常来说，对于复杂而耗时的任务，非分布式应用（运行在单个系统中）需要几个小时才能完成，而分布式应用通过使用所有系统涉及的计算能力可以在几分钟内完成。

通过将分布式应用配置为在更多系统上运行，可以进一步减少完成任务的时间。分布式应用正在运行的一组系统称为**集群**，而在集群中运行的每台机器被称为**节点**。

分布式应用有两部分， **Server（服务器）** 和 **Client（客户端）** 应用程序。服务器应用程序实际上是分布式的，并具有通用接口，以便客户端可以连接到集群中的任何服务器并获得相同的结果。 客户端应用程序是与分布式应用进行交互的工具。

![Zookeeper 概述](F:\Typora\images\1482983310991406.png)

### 2、分布式应用的优点

- **可靠性**  - 单个或几个系统的故障不会使整个系统出现故障。
- **可扩展性**  - 可以在需要时增加性能，通过添加更多机器，在应用程序配置中进行微小的更改，而不会有停机时间。
- **透明性**  - 隐藏系统的复杂性，并将其显示为单个实体/应用程序。



### 3、分布式应用的挑战

- **竞争条件**  - 两个或多个机器尝试执行特定任务，实际上只需在任意给定时间由单个机器完成。例如，共享资源只能在任意给定时间由单个机器修改。
- **死锁**  - 两个或多个操作等待彼此无限期完成。
- **不一致**  - 数据的部分失败。



### 4、什么是Apache ZooKeeper？

Apache ZooKeeper是由集群（节点组）使用的一种服务，用于在自身之间协调，并通过稳健的同步技术维护共享数据。ZooKeeper本身是一个分布式应用程序，为写入分布式应用程序提供服务。

ZooKeeper提供的常见服务如下 :

- **命名服务**  - 按名称标识集群中的节点。它类似于DNS，但仅对于节点。
- **配置管理**  - 加入节点的最近的和最新的系统配置信息。
- **集群管理**  - 实时地在集群和节点状态中加入/离开节点。
- **选举算法**  - 选举一个节点作为协调目的的leader。
- **锁定和同步服务**  - 在修改数据的同时锁定数据。此机制可帮助你在连接其他分布式应用程序（如Apache HBase）时进行自动故障恢复。
- **高度可靠的数据注册表**  - 即使在一个或几个节点关闭时也可以获得数据。

分布式应用程序提供了很多好处，但它们也抛出了一些复杂和难以解决的挑战。ZooKeeper框架提供了一个完整的机制来克服所有的挑战。竞争条件和死锁使用**故障安全同步方法**进行处理。另一个主要缺点是数据的不一致性，ZooKeeper使用**原子性**解析。



### 5、Zookeeper的好处

- **简单的分布式协调过程**
- **同步**  - 服务器进程之间的相互排斥和协作。此过程有助于Apache HBase进行配置管理。
- **有序的消息**
- **序列化**  - 根据特定规则对数据进行编码。确保应用程序运行一致。这种方法可以在MapReduce中用来协调队列以执行运行的线程。
- **可靠性**
- **原子性**  - 数据转移完全成功或完全失败，但没有事务是部分的。



## 2、基础

### 1、Zookeeper的架构

下面的图表。它描述了ZooKeeper的“客户端-服务器架构”。

![ZooKeeper的架构](F:\Typora\images\201612291344222238.jpg)

作为ZooKeeper架构的一部分的每个组件在下表中进行了说明。

| 部分             | 描述                                                         |
| ---------------- | ------------------------------------------------------------ |
| Client（客户端） | 客户端，我们的分布式应用集群中的一个节点，从服务器访问信息。对于特定的时间间隔，每个客户端向服务器发送消息以使服务器知道客户端是活跃的。类似地，当客户端连接时，服务器发送确认码。如果连接的服务器没有响应，客户端会自动将消息重定向到另一个服务器。 |
| Server（服务器） | 服务器，我们的ZooKeeper总体中的一个节点，为客户端提供所有的服务。向客户端发送确认码以告知服务器是活跃的。 |
| Ensemble         | ZooKeeper服务器组。形成ensemble所需的最小节点数为3。         |
| Leader           | 服务器节点，如果任何连接的节点失败，则执行自动恢复。Leader在服务启动时被选举。 |
| Follower         | 跟随leader指令的服务器节点。                                 |



### 2、层次命名空间

下图描述了用于内存表示的ZooKeeper文件系统的树结构。ZooKeeper节点称为 **znode** 。每个znode由一个名称标识，并用路径(/)序列分隔。

- 在图中，首先有一个由“/”分隔的znode。在根目录下，你有两个逻辑命名空间 **config** 和 **workers** 。
- **config** 命名空间用于集中式配置管理，**workers** 命名空间用于命名。
- 在 **config** 命名空间下，每个znode最多可存储1MB的数据。这与UNIX文件系统相类似，除了父znode也可以存储数据。这种结构的主要目的是存储同步数据并描述znode的元数据。此结构称为 **ZooKeeper数据模型**。

![分层命名空间](F:\Typora\images\201612291345162031-1587349756210.jpg)

ZooKeeper数据模型中的每个znode都维护着一个 **stat** 结构。一个stat仅提供一个znode的**元数据**。它由版本号，操作控制列表(ACL)，时间戳和数据长度组成。

- **版本号**  - 每个znode都有版本号，这意味着每当与znode相关联的数据发生变化时，其对应的版本号也会增加。当多个zookeeper客户端尝试在同一znode上执行操作时，版本号的使用就很重要。
- **操作控制列表(ACL)**  -  ACL基本上是访问znode的认证机制。它管理所有znode读取和写入操作。
- **时间戳**  - 时间戳表示创建和修改znode所经过的时间。它通常以毫秒为单位。ZooKeeper从“事务ID"(zxid)标识znode的每个更改。**Zxid** 是唯一的，并且为每个事务保留时间，以便你可以轻松地确定从一个请求到另一个请求所经过的时间。
- **数据长度**  - 存储在znode中的数据总量是数据长度。你最多可以存储1MB的数据。



### 3、Znode的类型

Znode被分为持久（persistent）节点，顺序（sequential）节点和临时（ephemeral）节点。

-  **持久节点**  - 即使在创建该特定znode的客户端断开连接后，持久节点仍然存在。默认情况下，除非另有说明，否则所有znode都是持久的。
-  **临时节点**   - 客户端活跃时，临时节点就是有效的。当客户端与ZooKeeper集合断开连接时，临时节点会自动删除。因此，只有临时节点不允许有子节点。如果临时节点被删除，则下一个合适的节点将填充其位置。临时节点在leader选举中起着重要作用。
-  **顺序节点**   - 顺序节点可以是持久的或临时的。当一个新的znode被创建为一个顺序节点时，ZooKeeper通过将10位的序列号附加到原始名称来设置znode的路径。例如，如果将具有路径 **/myapp** 的znode创建为顺序节点，则ZooKeeper会将路径更改为 **/myapp0000000001** ，并将下一个序列号设置为0000000002。如果两个顺序节点是同时创建的，那么ZooKeeper不会对每个znode使用相同的数字。顺序节点在锁定和同步中起重要作用。



### 4、Sessions（会话）

会话对于ZooKeeper的操作非常重要。会话中的请求按FIFO顺序执行。一旦客户端连接到服务器，将建立会话并向客户端分配**会话ID** 。

客户端以特定的时间间隔发送**心跳**以保持会话有效。如果ZooKeeper集合在超过服务器开启时指定的期间（会话超时）都没有从客户端接收到心跳，则它会判定客户端死机。

会话超时通常以毫秒为单位。当会话由于任何原因结束时，在该会话期间创建的临时节点也会被删除。



### 5、Watches（监视）

监视是一种简单的机制，使客户端收到关于ZooKeeper集合中的更改的通知。客户端可以在读取特定znode时设置Watches。Watches会向注册的客户端发送任何znode（客户端注册表）更改的通知。

Znode更改是与znode相关的数据的修改或znode的子项中的更改。只触发一次watches。如果客户端想要再次通知，则必须通过另一个读取操作来完成。当连接会话过期时，客户端将与服务器断开连接，相关的watches也将被删除。



## 3、工作流

一旦ZooKeeper集合启动，它将等待客户端连接。客户端将连接到ZooKeeper集合中的一个节点。它可以是leader或follower节点。一旦客户端被连接，节点将向特定客户端分配会话ID并向该客户端发送确认。如果客户端没有收到确认，它将尝试连接ZooKeeper集合中的另一个节点。 一旦连接到节点，客户端将以有规律的间隔向节点发送心跳，以确保连接不会丢失。

- **如果客户端想要读取特定的znode，**它将会向具有znode路径的节点发送**读取请求**，并且节点通过从其自己的数据库获取来返回所请求的znode。为此，在ZooKeeper集合中读取速度很快。
- **如果客户端想要将数据存储在ZooKeeper集合中**，则会将znode路径和数据发送到服务器。连接的服务器将该请求转发给leader，然后leader将向所有的follower重新发出写入请求。如果只有大部分节点成功响应，而写入请求成功，则成功返回代码将被发送到客户端。 否则，写入请求失败。绝大多数节点被称为 **Quorum** 



### Zookeeper集合中的节点

让我们分析在ZooKeeper集合中拥有不同数量的节点的效果。

- 如果我们有**单个节点**，则当该节点故障时，ZooKeeper集合将故障。它有助于“单点故障"，不建议在生产环境中使用。
- 如果我们有**两个节点**而一个节点故障，我们没有占多数，因为两个中的一个不是多数。
- 如果我们有**三个节点**而一个节点故障，那么我们有大多数，因此，这是最低要求。ZooKeeper集合在实际生产环境中必须至少有三个节点。
- 如果我们有**四个节点**而两个节点故障，它将再次故障。类似于有三个节点，额外节点不用于任何目的，因此，最好添加奇数的节点，例如3，5，7。

我们知道写入过程比ZooKeeper集合中的读取过程要贵，因为所有节点都需要在数据库中写入相同的数据。因此，对于平衡的环境拥有较少数量（例如3，5，7）的节点比拥有大量的节点要好。

下图描述了ZooKeeper工作流，后面的表说明了它的不同组件。

![Zookeeper - 工作流](F:\Typora\images\1482990578752713.png)

| 组件                              | 描述                                                         |
| --------------------------------- | ------------------------------------------------------------ |
| 写入（write）                     | 写入过程由leader节点处理。leader将写入请求转发到所有znode，并等待znode的回复。如果一半的znode回复，则写入过程完成。 |
| 读取（read）                      | 读取由特定连接的znode在内部执行，因此不需要与集群进行交互。  |
| 复制数据库（replicated database） | 它用于在zookeeper中存储数据。每个znode都有自己的数据库，每个znode在一致性的帮助下每次都有相同的数据。 |
| Leader                            | Leader是负责处理写入请求的Znode。                            |
| Follower                          | follower从客户端接收写入请求，并将它们转发到leader znode。   |
| 请求处理器（request processor）   | 只存在于leader节点。它管理来自follower节点的写入请求。       |
| 原子广播（atomic broadcasts）     | 负责广播从leader节点到follower节点的变化。                   |



## 4、leader选举

让我们分析如何在ZooKeeper集合中选举leader节点。考虑一个集群中有N个节点。leader选举的过程如下：



- 所有节点创建具有相同路径 /app/leader_election/guid_ 的顺序、临时节点。
- ZooKeeper集合将附加10位序列号到路径，创建的znode将是 /app/leader_election/guid_0000000001，/app/leader_election/guid_0000000002等。
- 对于给定的实例，在znode中创建最小数字的节点成为leader，而所有其他节点是follower。
- 每个follower节点监视下一个具有最小数字的znode。例如，创建znode/app/leader_election/guid_0000000008的节点将监视znode/app/leader_election/guid_0000000007，创建znode/app/leader_election/guid_0000000007的节点将监视znode/app/leader_election/guid_0000000006。
- 如果leader关闭，则其相应的znode/app/leader_electionN会被删除。
- 下一个在线follower节点将通过监视器获得关于leader移除的通知。
- 下一个在线follower节点将检查是否存在其他具有最小数字的znode。如果没有，那么它将承担leader的角色。否则，它找到的创建具有最小数字的znode的节点将作为leader。
- 类似地，所有其他follower节点选举创建具有最小数字的znode的节点作为leader。



leader选举是一个复杂的过程，但ZooKeeper服务使它非常简单。



## 5、安装使用

在 `Windows` 下安装使用



官网下载:https://zookeeper.apache.org/releases.html#download

打开 /bin 文件夹下的服务端：/zkServer.cmd



如果首次安装出现错误时，在 `zkServer.cmd` 文件中加入 `pause`，输出错误信息

显示错误为：没有找到 /conf 文件夹下的 `zoo.cfg` 文件，则将 `zoo_sample.cfg` 重命名为 `zoo.cfg` 即可。

再次打开即可使用



在客户端 `zkCli.cmd` 中存储数据，获取数据

```shell
[zk: localhost:2181(CONNECTED) 3] ls /
[zookeeper]
[zk: localhost:2181(CONNECTED) 4] create -e /cyy 1234
Created /cyy
[zk: localhost:2181(CONNECTED) 5] get /cyy
1234
```





# 3、Dubbo

## 1、概述

Dubbo是阿里巴巴在2011年开源的分布式服务框架，是SOA（Service-Oriented Architecture 面向服务的架构）服务化治理方案的核心框架。Dubbo主要提供三方面的功能：远程接口调用；负载均衡和容错；自动服务注册和发现。官方曾停止维护Dubbo很长一段时间，如今又开始维护，并将它贡献Apache开源基金会。也有很多其他第三方组织在更新和维护它，如当当在Dubbo的基础上开源了Dubbox。

![img](F:\Typora\images\2018090212233752.png)

网站应用的架构变化经历了一个从所有服务分布在一台服务器上（All in one 、单一应用架构）到 垂直应用架构 （MVC模式，按照各模块的职能划分）到分布式应用架构（RPC、按照服务不同分布在不同的服务器上）再到面向服务的架构（SOA，增加调度中心，负责集群的调度和管理）的过程。 Dubbo就是处在SOA架构阶段的一个远程服务调用框架。



## 2、系统结构

![img](F:\Typora\images\20180902123447214.png)

![img](F:\Typora\images\20180922152723994.png)

Dubbo系统分为五个部分：远程服务运行容器（Container），远程服务提供方（Provider）、注册中心（Register）、远程服务调用者（Consumer）、监控中心（Monitor）。

Dubbo服务调用过程：

- 服务提供方（Provider）所在的应用在容器中启动并运行（这个容器可以说是该应用部署的tomcat）
- 服务提供方（Provider）将自己要发布的服务注册到注册中心（Registry）
- 服务调用方（Consumer）启动后向注册中心订阅它想要调用的服务
- 注册中心（registry）存储着Provider注册的远程服务，并将其所管理的服务列表通知给服务调用方（Consumer），且注册中心和提供方和调用方之间均保持长连接，可以获取Provider发布的服务的变化情况，并将最新的服务列表推送给Consumer
- Consumer根据从注册中心获得的服务列表，根据软负载均衡算法选择一个服务提供者（Provider）进行远程服务调用，如果调用失败则选择另一台进行调用。（Consumer会缓存服务列表，即使注册中心宕机也不妨碍进行远程服务调用）
- 监控中心（Monitor）对服务的发布和订阅进行监控和统计服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心（Monitor）;Monitor可以选择Zookeeper、Redis或者Multiast注册中心等，后序介绍。



调用关系说明：

- 服务容器负责启动，加载，运行服务提供者。
- 服务提供者在启动时，向注册中心注册自己提供的服务。
- 服务消费者在启动时，向注册中心订阅自己所需的服务。
- 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。
- 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。
- 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。





Dubbo建议使用Zookeeper作为服务的注册中心

1. Zookeeper的作用

zookeeper用来注册服务和进行负载均衡，哪一个服务由哪一个机器来提供必需让调用者知道，简单来说就是ip地址和服务名称的对应关系。当然也可以  通过硬编码的方式把这种对应关系在调用方业务代码中实现，但是如果提供服务的机器挂掉调用者无法知晓，如果不更改代码会继续请求挂掉的机器提供服务。  zookeeper通过心跳机制可以检测挂掉的机器并将挂掉机器的ip和服务对应关系从列表中删除。至于支持高并发，简单来说就是横向扩展，在不更改代码 的情况通过添加机器来提高运算能力。通过添加新的机器向zookeeper注册服务，服务的提供者多了能服务的客户就多了。



2. dubbo：

是管理中间层的工具，在业务层到数据仓库间有非常多服务的接入和服务提供者需要调度，dubbo提供一个框架解决这个问题。

注意这里的dubbo只是一个框架，至于你架子上放什么是完全取决于你的，就像一个汽车骨架，你需要配你的轮子引擎。这个框架中要完成调度必须要有一个分布式的注册中心，储存所有服务的元数据，你可以用zk，也可以用别的，只是大家都用zk。

3. zookeeper和dubbo的关系

Dubbo的将注册中心进行抽象，是得它可以外接不同的存储媒介给注册中心提供服务，有ZooKeeper，Memcached，Redis等。
        引入了ZooKeeper作为存储媒介，也就把ZooKeeper的特性引进来。首先是负载均衡，单注册中心的承载能力是有限的，在流量达到一定程度的时 候就需要分流，负载均衡就是为了分流而存在的，一个ZooKeeper群配合相应的Web应用就可以很容易达到负载均衡；资源同步，单单有负载均衡还不  够，节点之间的数据和资源需要同步，ZooKeeper集群就天然具备有这样的功能；命名服务，将树状结构用于维护全局的服务地址列表，服务提供者在启动  的时候，向ZK上的指定节点/dubbo/${serviceName}/providers目录下写入自己的URL地址，这个操作就完成了服务的发布。 其他特性还有Mast选举，分布式锁等。

![img](F:\Typora\images\1183161-20170625200347445-383372044.png)





## 3、window下安装 dubbo

`dubbo` 本身并不是一个服务软件。它其实就是一个 jar 包，能够帮你的 `java` 程序连接到 `zookeeper` ，并利用 `zookeeper` 消费、提供服务。

但是为了让用户更好的管理监控众多的 `dubbo` 服务，官方提供了一个可视化的监控程序 `dubbo-admin` ，不过这个监控即使不装也不影响使用。

这里我们来安装一下：

1. 下载 dubbo-admin

   地址：https://github.com/apache/dubbo-admin/tree/master

2. 解压进入目录

   修改 dubbo-admin\src\main\resources\application.properties 指定 zookeeper 地址（如果zookeeper没有设置，则不需要修改）

3. 将项目达成 `jar` 包使用

   ```shell
   H:\dubbo-admin-master> mvn clean package -Dmaven.test.skip=true
   ```

   在 `target` 目录中生成了一个 `dubbo-admin-0.0.1-SNAPSHOT.jar` 

4. 先将 `zookeeper` 服务端打开，再将 `jar` 包运行起来

5. 访问浏览器 http://localhost:7001      ip+端口

   默认用户密码：	root/root

   ![image-20200422101324902](F:\Typora\images\image-20200422101324902.png)

dubbo-admin：是一个监控管理后台~，查看我们注册了哪些服务，哪些服务被消费了













# 4、Zookeeper+Dubbo+SpringBoot

## 1、RPC

### 什么是RPC？

RPC【Remote Procedure Call】是指远程过程调用，是一种进程间通信方式，它是一种技术的思想，而不是规范。它允许程序调用另一个地址空间（通常是共享网络上另一台机器）的过程或函数，而不用程序员显式编码这个远程调用的细节。即程序员无论是调用本地的还是远程的函数，本质上编写的调用代码相同。

也就是说两台服务器A、B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数/方法，由于不在一个内存空间，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。为什么要用RPC呢？就是无法在一个进程内，甚至一个计算机内通过本地调用的方式完成的需求，比如不同的系统间的通讯，甚至不同的组织之间的通讯，由于计算能力需要横向扩展，需要在多台机器组成的集群上部署应用。RPC就是要像调用本地的函数一样去调远程函数。



### RPC基本原理

[参考博客](https://www.jianshu.com/p/2accc2840a1b)

![image-20200421200612178](F:\Typora\images\image-20200421200612178.png)



### 步骤解析

![image-20200421200702176](F:\Typora\images\image-20200421200702176.png)



**RPC两个核心模块：通讯、序列化**



## 2、项目实战

项目路径：<a href="">E:\workspace\workspace-test\Distributed</a>

步骤：

前提：zookeeper要提前开启

1. 提供者提供服务

   1. 导入依赖
   2. 配置注册中心的地址，以及服务发现名，和要扫描的包
   3. 在想要被注册的服务上面，增加一个注解 @Service

   ![image-20200422222002960](F:\Typora\images\image-20200422222002960.png)

2. 消费者如何消费

   1. 导入依赖
   2. 配置注册中心的地址，配置自己的服务名
   3. 从远程注入服务~@Reference，包路径也需要一致





# 5、SpringCloud+Eureka

## 1、Eureka是什么

Eureka是Netflix的一个子模块，也是核心模块之一。Eureka是一个基于REST的服务，用于定位服务，以实现云端中间层服务发现和故障转移。服务注册与发现对于微服务架构来说是非常重要的，有了服务发现与注册，只需要使用服务的标识符，就可以访问到服务，而不需要修改服务调用的配置文件了。



**Eureka包含组件**

Eureka包含两个组件：Eureka Server和Eureka Client



**Eureka Server提供服务注册服务**

各个节点启动后，会在EurekaServer中进行注册，这样EurekaServer中的服务注册表中将会存储所有可用服务节点的信息，服务节点的信息可以在界面中直观看到。

**Eureka Client是一个java客户端，用于简化Eureka的交互，客户端同时也具备一个内置的、使用轮询（round-robin）负载算法的负载均衡器。**

在应用启动后，将会向Eureka Sever发送心跳（默认周期为30秒）。如果Eureka Server在多个心跳周期没有接收到某个节点的心跳，EurekaServer 将会从服务注册表中把这个服务节点移除（默认90秒）



## 2、ACP原则

首先我们来讲一下ACP原则，便于一会的产品对比。

<img src="F:\Typora\images\image-20200424142342942.png" alt="image-20200424142342942" style="zoom:80%;" />

CAP是**Consistency、Availablity和Partition-tolerance**的缩写。

分别是指：

1.**一致性（Consistency）**：每次读操作都能保证返回的是最新数据；

2.**可用性（Availablity）**：任何一个没有发生故障的节点，会在合理的时间内返回一个正常的结果；

3.**分区容忍性（Partition-torlerance）**：当节点间出现网络分区，照样可以提供服务。

**注意：在分布式系统中，CAP只能三选二，没有任何分布式架构三种都满足。**

**分布式架构一般要么CP要么AP，因为分布式一定要保证分区容忍性。**



**为什么说是三进二呢？**

**1：满足C和A，那么P能不能满足呢？**

满足C需要所有的服务器的数据要一样，也就是说要实现数据的同步，那么同步要不要时间？肯定是要的，并且机器越多，同步的时间肯定越慢，这里问题就来了，我们同时也满足了A，也就是说，我要同步时间短才行。这样的话，机器就不能太多了，也就是说P是满足不了的

**2：满足C和P，那么A能不能满足呢？**

满足P需要很多服务器，假设有1000台服务器，同时满足了C，也就是说要保证每台机器的数据都一样，那么同步的时间可就很大，在这种情况下，我们肯定是不能保证用户随时访问每台服务器获取到的数据都是最新的，想要获取最新的，可以，你就等吧，等全部同步完了，你就可以获取到了，但是我们的A要求短时间就可以拿到想要的数据啊，这不就是矛盾了，所以说这里A是满足不了了

**3：满足A和P，那么C能不能满足呢？**

自行考虑一下。

**与Zookeeper的对比**

为什么要讲CAP呢，因为我们下面要对Dubbo中的Zookeeper和SpringCloud中的Eureka进行对比了。

与Eureka功能类似的是Dubbo的注册中心，比如Zookeeper。

**那么Eureka和Zookeeper区别在哪里，我们又如何进行选择呢？**

Eureka开发是吸取了Zookeeper的所有教训后出现的一个产品：

Netflix在设计Eureka时遵守的是AP原则。

Zookeeper使用的是CP原则。

下面我们来分析一下：

**Zookeeper保证CP**

当向注册中心查询服务列表时，我们可以容忍注册中心返回的是几分钟以前的注册信息，但不能接受服务直接down掉不可用。也就是说，服务注册功能对可用性的要求要高于一致性。但是zk会出现这样一种情况，当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30 ~ 120s, 且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪。在云部署的环境下，因网络问题使得zk集群失去master节点是较大概率会发生的事，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。

**Eureka保证AP**

Eureka看明白了这一点，因此在设计时就优先保证可用性。Eureka各个节点都是平等的，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而Eureka的客户端在向某个Eureka注册或时如果发现连接失败，则会自动切换至其它节点，只要有一台Eureka还在，就能保证注册服务可用(保证可用性)，只不过查到的信息可能不是最新的(不保证强一致性)。除此之外，Eureka还有一种自我保护机制，如果在15分钟内超过85%的节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况：

1. Eureka不再从注册列表中移除因为长时间没收到心跳而应该过期的服务

2. Eureka仍然能够接受新服务的注册和查询请求，但是不会被同步到其它节点上(即保证当前节点依然可用)

3. 当网络稳定时，当前实例新的注册信息会被同步到其它节点中

因此， Eureka可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像zookeeper那样使整个注册服务瘫痪。



小总结

Eureka作为单纯的服务注册中心来说要比zookeeper更加“专业”，因为注册服务更重要的是可用性，我们可以接受短期内达不到一致性的状况。不过Eureka目前1.X版本的实现是基于servlet的java web应用，它的极限性能肯定会受到影响。期待正在开发之中的2.X版本能够从servlet中独立出来成为单独可部署执行的服务。
